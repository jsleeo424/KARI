<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Activity Grammars for Temporal Action Segmentation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://cvlab.postech.ac.kr/lab/research.php">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://cvlab.postech.ac.kr/research/FUTR/">
            FUTR
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Activity Grammars for Temporal Action Segmentation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://gongda0e.github.io/">Dayoung Gong</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://jsleeo424.github.io">Joonseok Lee</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://hesedjds.github.io">Deunsol Jung</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://suhakwak.github.io">Suha Kwak</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://cvlab.postech.ac.kr/~mcho">Minsu Cho</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Pohang University of Science and Technology (POSTECH)</span><br>
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/gongda0e/neurips23_supp"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://mega.nz/file/O6wXlSTS#wcEoDT4Ctq5HRq_hV-aWeVF1_JB3cacQBQqOLjCIbc8"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Sequence prediction on temporal data requires the ability to understand compositional structures of multi-level 
            semantics beyond individual and contextual properties. The task of temporal action segmentation, which aims at 
            translating an untrimmed activity video into a sequence of action segments, remains challenging for this reason.
          </p>
          <p>
            This paper addresses the problem by introducing an effective activity grammar to guide neural predictions for 
            temporal action segmentation. We propose a novel grammar induction algorithm that extracts a powerful context-free
            grammar from action sequence data. We also develop an efficient generalized parser that transforms frame-level 
            probability distributions into a reliable sequence of actions according to the induced grammar with recursive rules. 
            Our approach can be combined with any neural network for temporal action segmentation to enhance the sequence 
            prediction and discover its compositional structure.
          </p>
          <p>
            Experimental results demonstrate that our method significantly improves temporal action
            segmentation in terms of both performance and interpretability on two standard
            benchmarks, Breakfast and 50 Salads.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="Overall Pipeline">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <h2 class="title is-3">Overall Pipeline</h2>
        <img src="./static/images/pipeline35.jpg"
                type="images/jpg" />
      <div class="content has-text-justified">
        Illustration of the overall architecture of the proposed method.
        (a) KARI induces an activity grammar from action sequences in the training data,
        (b) BEP parses neural predictions from the off-the-shelf temporal action segmentation model given a video 
        by using the KARI-induced grammar, and 
        (c) the final output of optimal action sequences and lengths is achieved through segmentation
        optimization. It is best viewed in color.
      </div>
    </div>
  </div>
</section>

<section class="video">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <h2 class="title is-3">Video</h2><br>
      <div class="columns is-centered has-text-centered">
        <div class="publication-video">
          <iframe src="./static/videos/PPT.mp4"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="Experimental results">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">Experiments</h2>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
        <figure>
          <img src="./static/images/sota1.png"
                  type="images/png" />
          <figcaption><b>The performance comparison on 50 Salads</b></figcaption><br>
        </figure>
        <figure>
          <img src="./static/images/sota2.png"
                  type="images/png" />
          <figcaption><b>The performance comparison on Breakfast</b></figcaption><br>
        </figure>
      </div>
      </div>
    </div>
  </div>
</section>

<section class="Qualitative results">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">Qualitative Results</h2>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <figure>
          <img src="./static/images/50salads_qual.jpg"
                  type="images/jpg" />
          <figcaption><b>Making Salads in 50 Salads</b></figcaption><br>
        </figure>
        <figure>
          <img src="./static/images/breakfast_qual.jpg"
                  type="images/jpg" />
          <figcaption><b>Frying eggs in Breakfast</b></figcaption><br><br>
        </figure>
      </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Related Links</h2>
        <div class="content has-text-justified">
          <ul>
            <li>S. Qi, B. Jia, and S.-C. Zhu. <a href="https://arxiv.org/pdf/1806.03497.pdf">Generalized earley parser</a>: Bridging symbolic grammars and
              sequence data for future prediction. In Proc. International Conference on Machine Learning
              (ICML), pages 4171–4179. PMLR, 2018.</li><br>
            <li>Z. Solan, D. Horn, E. Ruppin, and S. Edelman. <a href="https://www.pnas.org/doi/10.1073/pnas.0409746102">
              Unsupervised learning of natural languages.</a>
              Proceedings of the National Academy of Sciences, 102(33):11629–11634, 2005.</li><br>
            <li>J. Earley. <a href="https://dl.acm.org/doi/pdf/10.1145/362007.362035">An efficient context-free parsing algorithm.</a> 
              Communications of the ACM, 13(2):94–102, 1970.</li><br>
            <li>F. Yi, H. Wen, and T. Jiang. <a href="https://arxiv.org/pdf/2110.08568.pdf">
              Asformer: Transformer for action segmentation.</a> In Proc. British
              Machine Vision Conference (BMVC), 2021.</li><br>
            <li>Y. A. Farha and J. Gall. <a href="https://arxiv.org/pdf/1903.01945.pdf">Ms-tcn: Multi-stage temporal convolutional network for action
              segmentation.</a> In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
              pages 3575–3584, 2019.</li><br>
            <li>H. Kuehne, A. Arslan, and T. Serre. <a href="https://ieeexplore.ieee.org/document/6909500">The language of actions: Recovering the syntax and
              semantics of goal-directed human activities.</a> In Proc. IEEE Conference on Computer Vision and
              Pattern Recognition (CVPR), pages 780–787, 2014.</li><br>
            <li>S. Stein and S. J. McKenna. <a href="https://dl.acm.org/doi/10.1145/2493432.2493482">Combining embedded accelerometers with computer vision for
              recognizing food preparation activities.</a> In Proceedings of the 2013 ACM international joint
              conference on Pervasive and ubiquitous computing, pages 729–738, 2013.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title has-text-centered">BibTeX</h2>
    <pre><code>@inproceedings{gong2023activity,
      title={Activity Grammars for Temporal Action Segmentation},
      author={Dayoung Gong and Joonseok Lee and Deunsol Jung and Suha Kwak and Minsu Cho},
      booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
      year={2023},
      url={https://openreview.net/forum?id=oOXZ5JEjPb}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adpated from the following <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
